{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GOAL:\n",
    "\n",
    "This note book is aimed to simulate a step by step process of how I came to think about main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRE-REQUISITES:\n",
    "- have a data.csv containing transactions downloaded with (this is the file you want to load to talk with the LLM chain)\n",
    "- define `.env` as appropriate, refer to the variables needed in `configs.py`, also place your API tokens here and load into `configs.py`\n",
    "- have the minimum RAM required to run the .gguf model (5~6 GB RAM)\n",
    "  - due to hardware constraint the `.gguf` model is used for mistral-7b which suffers quality loss in exchange for less hardware resource usage\n",
    "  - you can opt to extend the code to load full models from hugging face / ollama if you do not face such hardware constraints\n",
    "- the python version used is `3.11.5`\n",
    "- have a conda environment or equivalent created e.g. `conda create -n venv python=3.11.5`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTS / SET-UPs\n",
    "\n",
    "setup is done for logger and database dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: conda: command not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain==0.3.15 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from -r requirements.txt (line 1)) (0.3.15)\n",
      "Requirement already satisfied: transformers==4.48.1 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (4.48.1)\n",
      "Requirement already satisfied: torch==2.5.1 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (2.5.1)\n",
      "Requirement already satisfied: langchain-community==0.3.15 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from -r requirements.txt (line 4)) (0.3.15)\n",
      "Requirement already satisfied: langchain-experimental==0.3.4 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from -r requirements.txt (line 5)) (0.3.4)\n",
      "Requirement already satisfied: pandas==2.2.3 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from -r requirements.txt (line 6)) (2.2.3)\n",
      "Requirement already satisfied: langchain-huggingface==0.1.2 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from -r requirements.txt (line 7)) (0.1.2)\n",
      "Requirement already satisfied: pydantic==2.10.5 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from -r requirements.txt (line 8)) (2.10.5)\n",
      "Requirement already satisfied: accelerate==1.3.0 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from -r requirements.txt (line 9)) (1.3.0)\n",
      "Requirement already satisfied: llama-cpp-python==0.3.6 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from -r requirements.txt (line 10)) (0.3.6)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from langchain==0.3.15->-r requirements.txt (line 1)) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from langchain==0.3.15->-r requirements.txt (line 1)) (2.0.37)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from langchain==0.3.15->-r requirements.txt (line 1)) (3.11.11)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.31 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from langchain==0.3.15->-r requirements.txt (line 1)) (0.3.31)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from langchain==0.3.15->-r requirements.txt (line 1)) (0.3.5)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from langchain==0.3.15->-r requirements.txt (line 1)) (0.3.1)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from langchain==0.3.15->-r requirements.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from langchain==0.3.15->-r requirements.txt (line 1)) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from langchain==0.3.15->-r requirements.txt (line 1)) (9.0.0)\n",
      "Requirement already satisfied: filelock in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from transformers==4.48.1->-r requirements.txt (line 2)) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from transformers==4.48.1->-r requirements.txt (line 2)) (0.27.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from transformers==4.48.1->-r requirements.txt (line 2)) (24.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from transformers==4.48.1->-r requirements.txt (line 2)) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from transformers==4.48.1->-r requirements.txt (line 2)) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from transformers==4.48.1->-r requirements.txt (line 2)) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from transformers==4.48.1->-r requirements.txt (line 2)) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from torch==2.5.1->-r requirements.txt (line 3)) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from torch==2.5.1->-r requirements.txt (line 3)) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from torch==2.5.1->-r requirements.txt (line 3)) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from torch==2.5.1->-r requirements.txt (line 3)) (2024.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from torch==2.5.1->-r requirements.txt (line 3)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from torch==2.5.1->-r requirements.txt (line 3)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from torch==2.5.1->-r requirements.txt (line 3)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from torch==2.5.1->-r requirements.txt (line 3)) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from torch==2.5.1->-r requirements.txt (line 3)) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from torch==2.5.1->-r requirements.txt (line 3)) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from torch==2.5.1->-r requirements.txt (line 3)) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from torch==2.5.1->-r requirements.txt (line 3)) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from torch==2.5.1->-r requirements.txt (line 3)) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from torch==2.5.1->-r requirements.txt (line 3)) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from torch==2.5.1->-r requirements.txt (line 3)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from torch==2.5.1->-r requirements.txt (line 3)) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from torch==2.5.1->-r requirements.txt (line 3)) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from torch==2.5.1->-r requirements.txt (line 3)) (1.13.1)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from langchain-community==0.3.15->-r requirements.txt (line 4)) (0.6.7)\n",
      "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from langchain-community==0.3.15->-r requirements.txt (line 4)) (0.4.0)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from langchain-community==0.3.15->-r requirements.txt (line 4)) (2.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from pandas==2.2.3->-r requirements.txt (line 6)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from pandas==2.2.3->-r requirements.txt (line 6)) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from pandas==2.2.3->-r requirements.txt (line 6)) (2025.1)\n",
      "Requirement already satisfied: sentence-transformers>=2.6.0 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from langchain-huggingface==0.1.2->-r requirements.txt (line 7)) (3.3.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from pydantic==2.10.5->-r requirements.txt (line 8)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from pydantic==2.10.5->-r requirements.txt (line 8)) (2.27.2)\n",
      "Requirement already satisfied: psutil in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from accelerate==1.3.0->-r requirements.txt (line 9)) (6.1.1)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from llama-cpp-python==0.3.6->-r requirements.txt (line 10)) (5.6.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from sympy==1.13.1->torch==2.5.1->-r requirements.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.15->-r requirements.txt (line 1)) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.15->-r requirements.txt (line 1)) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.15->-r requirements.txt (line 1)) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.15->-r requirements.txt (line 1)) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.15->-r requirements.txt (line 1)) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.15->-r requirements.txt (line 1)) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.15->-r requirements.txt (line 1)) (1.18.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.15->-r requirements.txt (line 4)) (3.25.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.15->-r requirements.txt (line 4)) (0.9.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from jinja2->torch==2.5.1->-r requirements.txt (line 3)) (3.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.31->langchain==0.3.15->-r requirements.txt (line 1)) (1.33)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain==0.3.15->-r requirements.txt (line 1)) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain==0.3.15->-r requirements.txt (line 1)) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain==0.3.15->-r requirements.txt (line 1)) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain==0.3.15->-r requirements.txt (line 1)) (0.23.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community==0.3.15->-r requirements.txt (line 4)) (1.0.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas==2.2.3->-r requirements.txt (line 6)) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from requests<3,>=2->langchain==0.3.15->-r requirements.txt (line 1)) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from requests<3,>=2->langchain==0.3.15->-r requirements.txt (line 1)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from requests<3,>=2->langchain==0.3.15->-r requirements.txt (line 1)) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from requests<3,>=2->langchain==0.3.15->-r requirements.txt (line 1)) (2024.12.14)\n",
      "Requirement already satisfied: scikit-learn in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from sentence-transformers>=2.6.0->langchain-huggingface==0.1.2->-r requirements.txt (line 7)) (1.6.1)\n",
      "Requirement already satisfied: scipy in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from sentence-transformers>=2.6.0->langchain-huggingface==0.1.2->-r requirements.txt (line 7)) (1.15.1)\n",
      "Requirement already satisfied: Pillow in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from sentence-transformers>=2.6.0->langchain-huggingface==0.1.2->-r requirements.txt (line 7)) (11.1.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.3.15->-r requirements.txt (line 1)) (3.1.1)\n",
      "Requirement already satisfied: anyio in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain==0.3.15->-r requirements.txt (line 1)) (4.8.0)\n",
      "Requirement already satisfied: httpcore==1.* in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain==0.3.15->-r requirements.txt (line 1)) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain==0.3.15->-r requirements.txt (line 1)) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.31->langchain==0.3.15->-r requirements.txt (line 1)) (3.0.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.15->-r requirements.txt (line 4)) (1.0.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface==0.1.2->-r requirements.txt (line 7)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface==0.1.2->-r requirements.txt (line 7)) (3.5.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/bot/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain==0.3.15->-r requirements.txt (line 1)) (1.3.1)\n",
      "Package                  Version\n",
      "------------------------ -----------\n",
      "accelerate               1.3.0\n",
      "aiohappyeyeballs         2.4.4\n",
      "aiohttp                  3.11.11\n",
      "aiosignal                1.3.2\n",
      "annotated-types          0.7.0\n",
      "anyio                    4.8.0\n",
      "asttokens                3.0.0\n",
      "attrs                    24.3.0\n",
      "certifi                  2024.12.14\n",
      "charset-normalizer       3.4.1\n",
      "comm                     0.2.2\n",
      "dataclasses-json         0.6.7\n",
      "debugpy                  1.8.12\n",
      "decorator                5.1.1\n",
      "diskcache                5.6.3\n",
      "executing                2.2.0\n",
      "filelock                 3.17.0\n",
      "frozenlist               1.5.0\n",
      "fsspec                   2024.12.0\n",
      "greenlet                 3.1.1\n",
      "h11                      0.14.0\n",
      "httpcore                 1.0.7\n",
      "httpx                    0.28.1\n",
      "httpx-sse                0.4.0\n",
      "huggingface-hub          0.27.1\n",
      "idna                     3.10\n",
      "ipykernel                6.29.5\n",
      "ipython                  8.31.0\n",
      "jedi                     0.19.2\n",
      "Jinja2                   3.1.5\n",
      "joblib                   1.4.2\n",
      "jsonpatch                1.33\n",
      "jsonpointer              3.0.0\n",
      "jupyter_client           8.6.3\n",
      "jupyter_core             5.7.2\n",
      "langchain                0.3.15\n",
      "langchain-community      0.3.15\n",
      "langchain-core           0.3.31\n",
      "langchain-experimental   0.3.4\n",
      "langchain-huggingface    0.1.2\n",
      "langchain-text-splitters 0.3.5\n",
      "langsmith                0.3.1\n",
      "llama_cpp_python         0.3.6\n",
      "MarkupSafe               3.0.2\n",
      "marshmallow              3.25.1\n",
      "matplotlib-inline        0.1.7\n",
      "mpmath                   1.3.0\n",
      "multidict                6.1.0\n",
      "mypy-extensions          1.0.0\n",
      "nest-asyncio             1.6.0\n",
      "networkx                 3.4.2\n",
      "numpy                    1.26.4\n",
      "nvidia-cublas-cu12       12.4.5.8\n",
      "nvidia-cuda-cupti-cu12   12.4.127\n",
      "nvidia-cuda-nvrtc-cu12   12.4.127\n",
      "nvidia-cuda-runtime-cu12 12.4.127\n",
      "nvidia-cudnn-cu12        9.1.0.70\n",
      "nvidia-cufft-cu12        11.2.1.3\n",
      "nvidia-curand-cu12       10.3.5.147\n",
      "nvidia-cusolver-cu12     11.6.1.9\n",
      "nvidia-cusparse-cu12     12.3.1.170\n",
      "nvidia-nccl-cu12         2.21.5\n",
      "nvidia-nvjitlink-cu12    12.4.127\n",
      "nvidia-nvtx-cu12         12.4.127\n",
      "orjson                   3.10.15\n",
      "packaging                24.2\n",
      "pandas                   2.2.3\n",
      "parso                    0.8.4\n",
      "pexpect                  4.9.0\n",
      "pillow                   11.1.0\n",
      "pip                      24.3.1\n",
      "platformdirs             4.3.6\n",
      "prompt_toolkit           3.0.50\n",
      "propcache                0.2.1\n",
      "psutil                   6.1.1\n",
      "ptyprocess               0.7.0\n",
      "pure_eval                0.2.3\n",
      "pydantic                 2.10.5\n",
      "pydantic_core            2.27.2\n",
      "pydantic-settings        2.7.1\n",
      "Pygments                 2.19.1\n",
      "python-dateutil          2.9.0.post0\n",
      "python-dotenv            1.0.1\n",
      "pytz                     2024.2\n",
      "PyYAML                   6.0.2\n",
      "pyzmq                    26.2.0\n",
      "regex                    2024.11.6\n",
      "requests                 2.32.3\n",
      "requests-toolbelt        1.0.0\n",
      "safetensors              0.5.2\n",
      "scikit-learn             1.6.1\n",
      "scipy                    1.15.1\n",
      "sentence-transformers    3.3.1\n",
      "setuptools               75.8.0\n",
      "six                      1.17.0\n",
      "sniffio                  1.3.1\n",
      "SQLAlchemy               2.0.37\n",
      "stack-data               0.6.3\n",
      "sympy                    1.13.1\n",
      "tenacity                 9.0.0\n",
      "threadpoolctl            3.5.0\n",
      "tokenizers               0.21.0\n",
      "torch                    2.5.1\n",
      "tornado                  6.4.2\n",
      "tqdm                     4.67.1\n",
      "traitlets                5.14.3\n",
      "transformers             4.48.1\n",
      "triton                   3.1.0\n",
      "typing_extensions        4.12.2\n",
      "typing-inspect           0.9.0\n",
      "tzdata                   2025.1\n",
      "urllib3                  2.3.0\n",
      "wcwidth                  0.2.13\n",
      "wheel                    0.45.1\n",
      "yarl                     1.18.3\n",
      "zstandard                0.23.0\n"
     ]
    }
   ],
   "source": [
    "! conda activate myprojects # or replace with your desired environemnt name\n",
    "! pip install -r requirements.txt\n",
    "! pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import time\n",
    "from typing import Any, List, Optional, Tuple, Type, Union\n",
    "from llama_cpp import Llama\n",
    "\n",
    "from langchain_experimental.sql import SQLDatabaseChain, SQLDatabaseSequentialChain\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "from langchain.schema.cache import BaseCache\n",
    "from langchain.callbacks.base import Callbacks\n",
    "from langchain.sql_database import SQLDatabase\n",
    "from langchain.schema import BaseOutputParser\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.prompts import BasePromptTemplate, PromptTemplate\n",
    "from pydantic import Field\n",
    "\n",
    "from classes import GracefulSQLDatabaseChain\n",
    "from mydatabase import initialize_database\n",
    "from utils import BenchmarkReport, setup_logger, truncate_conversation_history\n",
    "from myprompts import ALL_PROMPT_STRINGS, DEFAULT_SQLITE_PROMPT, prompt_template_generator, _sqlite_prompt1, _sqlite_prompt2, _sqlite_prompt3\n",
    "import myprompts\n",
    "from configs import DATABASE_PATH, DATABASE_URL, DEFAULT_CHAT_OUTPUT_FILEPATH, DEFAULT_MODEL_PATH\n",
    "\n",
    "\n",
    "# Load the model\n",
    "from main import load_local_model, DEFAULT_CONTEXT_WINDOW_SIZE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing database 'data.db' has been deleted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-24 15:45:47,813 - mydatabase - INFO - Loaded data from data.csv into 'transactions' table.\n",
      "2025-01-24 15:45:47,843 - mydatabase - INFO - Loaded data from clients.csv into 'clients' table.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from data.csv has been successfully loaded into 'transactions' table.\n",
      "Data from clients.csv has been successfully loaded into 'clients' table.\n"
     ]
    }
   ],
   "source": [
    "from main import test_database_context\n",
    "\n",
    "\n",
    "logger = setup_logger(\"jupyter_notebook\", \"jupyter.log\", level=logging.INFO)\n",
    "\n",
    "if os.path.exists(DATABASE_PATH):\n",
    "    os.remove(DATABASE_PATH)\n",
    "    print(f\"Existing database '{DATABASE_PATH}' has been deleted.\")\n",
    "# Reinitialize the database\n",
    "initialize_database()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step by step: Build > Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import CustomLlamaLLM\n",
    "from main import load_database_connection\n",
    "from main import create_llm_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the model from local storage and wrap it with subclass of langchain's `LLM` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from ./models/mistral-7b-instruct-v0.1.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: control token:      2 '</s>' is not marked as EOG\n",
      "llm_load_vocab: control token:      1 '<s>' is not marked as EOG\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: tensor 'token_embd.weight' (q4_K) (and 290 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "llm_load_tensors:   CPU_Mapped model buffer size =  4165.37 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_seq_max     = 1\n",
      "llama_new_context_with_model: n_ctx         = 4096\n",
      "llama_new_context_with_model: n_ctx_per_seq = 4096\n",
      "llama_new_context_with_model: n_batch       = 512\n",
      "llama_new_context_with_model: n_ubatch      = 512\n",
      "llama_new_context_with_model: flash_attn    = 0\n",
      "llama_new_context_with_model: freq_base     = 10000.0\n",
      "llama_new_context_with_model: freq_scale    = 1\n",
      "llama_new_context_with_model: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init: kv_size = 4096, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init:        CPU KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   296.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.1', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
      "Using fallback chat format: llama-2\n",
      "2025-01-24 15:45:49,255 - main - INFO - Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "llama_model = load_local_model(DEFAULT_MODEL_PATH, DEFAULT_CONTEXT_WINDOW_SIZE)\n",
    "# Initialize the custom Llama LLM\n",
    "llm = CustomLlamaLLM(llama_model, DEFAULT_CONTEXT_WINDOW_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After initializing the database in the setup, load the database from path (this assumes a local sqllite database, but could be extended to use other connectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-24 15:45:49,403 - main - INFO - Database connected successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sql_database = load_database_connection(DATABASE_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a prompt template appropraite for your SQL assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SQL prompt template\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "You are an assistant for a banking client. Answer questions based on the provided database schema and data.\n",
    "\n",
    "{table_info}\n",
    "\n",
    "User Query: {input}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"table_info\", \"input\"],\n",
    "    template=PROMPT_TEMPLATE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The build phase is complete after building the SQL LLM chain with all the above dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-24 15:47:07,660 - main - INFO - Banking assistant created successfully.\n"
     ]
    }
   ],
   "source": [
    "llm_chain = create_llm_chain(\n",
    "    database=sql_database,\n",
    "    llm=llm,\n",
    "    prompt= DEFAULT_SQLITE_PROMPT,\n",
    "    database_chain_cls=SQLDatabaseChain\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we finally run the LLM + Database = SQL LLM Chain, this assumes a pre-defined set of questions.\n",
    "Order matters! This is because conversation history is preserved in the LLM context window in sequential manner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Query: How many transactions are in the database?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SQLDatabaseChain chain...\u001b[0m\n",
      "How many transactions are in the database?\n",
      "SQLQuery:"
     ]
    }
   ],
   "source": [
    "# Simulate user queries to the assistant\n",
    "questions = [\n",
    "    \"How many transactions are in the database?\",\n",
    "    \"List all transactions by client ID C001.\",\n",
    "    \"What is the total amount spent by client ID C002?\"\n",
    "]\n",
    "\n",
    "for ques in questions:\n",
    "    print(f\"User Query: {ques}\")\n",
    "    response = llm_chain.run(ques)\n",
    "    print(f\"Assistant Response: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the main loop with minimal code\n",
    "\n",
    "running the chat loop as though you are speaking to the SQL DB Chain LLM in real-time, \n",
    "<br>\n",
    "**type \"exit\" to exit the loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from main import main_run_loop\n",
    "# main_run_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USAGE with CLI on pure python scripts\n",
    "\n",
    "**NOTE: --simulate and --benchmark cannot be used together. The script will throw an error if both are provided**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default usage:\n",
    "! python main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simulate a chat run against a pre-defiuned set of questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python main.py --simulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO RUN Benchmarking tests against a pre-defined set of questions and sets of pre-defined configurations on the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python main.py --benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If memory is enabled, which means the LLM chain is not stateless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python main.py --memory\n",
    "! python main.py --simulate --memory\n",
    "! python main.py --benchmark --memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JUSTIFICATIONS AND EXPLANATIONS\n",
    "FOR CONSIDERATIONS AND JUSTIFCATIONS refer to `README.md`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
